import torch
import numpy as np
import soundfile as sf
from qwen_asr import Qwen3ASRModel
from config import RESULT_DIR
from audio_utils import convert_to_wav, get_audio_duration
from opencc import OpenCC
from typing import List, Dict, Any
from pathlib import Path

# ç¹ç°¡è½‰æ›å™¨
cc = OpenCC('s2twp')

# ============================================
# éŸ³è¨Šåˆ†æ®µ
# ============================================

def split_audio_by_silence(
    audio_path: str,
    target_duration: float = 120.0,
    max_duration: float = 180.0,
    silence_threshold_db: float = -40.0,
    min_silence_duration: float = 0.3,
    frame_duration: float = 0.02,
) -> List[tuple]:
    """
    ä¾éœéŸ³æ®µåˆ‡åˆ†éŸ³è¨Šï¼Œå›å‚³å„æ®µ (start_sec, end_sec) æ¸…å–®ã€‚

    Args:
        audio_path: WAV æª”æ¡ˆè·¯å¾‘
        target_duration: ç›®æ¨™ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
        max_duration: æœ€å¤§ç‰‡æ®µé•·åº¦ï¼ˆç§’ï¼‰
        silence_threshold_db: éœéŸ³åˆ¤å®šé–€æª»ï¼ˆdBï¼‰
        min_silence_duration: æœ€å°éœéŸ³æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
        frame_duration: RMS è¨ˆç®—çª—å£å¤§å°ï¼ˆç§’ï¼‰

    Returns:
        [(start, end), ...] å„æ®µçš„èµ·è¨–æ™‚é–“
    """
    total_duration = get_audio_duration(audio_path)

    # çŸ­éŸ³è¨Šä¸åˆ‡åˆ†
    if total_duration <= target_duration:
        return [(0.0, total_duration)]

    print(f"ğŸ” åµæ¸¬éœéŸ³æ®µ... (éŸ³è¨Šé•·åº¦: {total_duration:.1f}s)")

    # è®€å–éŸ³è¨Šè¨ˆç®— RMS èƒ½é‡
    data, sr = sf.read(audio_path, dtype='float32')
    if data.ndim > 1:
        data = data.mean(axis=1)  # æ··ç‚ºå–®è²é“

    frame_size = int(sr * frame_duration)
    num_frames = len(data) // frame_size
    threshold_linear = 10 ** (silence_threshold_db / 20.0)

    # è¨ˆç®—æ¯å€‹ frame çš„ RMS
    rms_values = []
    for i in range(num_frames):
        frame = data[i * frame_size : (i + 1) * frame_size]
        rms = np.sqrt(np.mean(frame ** 2))
        rms_values.append(rms)

    # åµæ¸¬éœéŸ³å€æ®µ
    silence_regions = []
    in_silence = False
    silence_start = 0

    for i, rms in enumerate(rms_values):
        time_sec = i * frame_duration
        if rms < threshold_linear:
            if not in_silence:
                in_silence = True
                silence_start = time_sec
        else:
            if in_silence:
                silence_end = time_sec
                duration = silence_end - silence_start
                if duration >= min_silence_duration:
                    silence_regions.append((silence_start, silence_end))
                in_silence = False

    # çµå°¾éœéŸ³
    if in_silence:
        silence_end = num_frames * frame_duration
        if silence_end - silence_start >= min_silence_duration:
            silence_regions.append((silence_start, silence_end))

    print(f"   æ‰¾åˆ° {len(silence_regions)} å€‹éœéŸ³å€æ®µ")

    # ä¾éœéŸ³æ®µé¸æ“‡åˆ‡åˆ†é»
    chunks = []
    current_start = 0.0

    while current_start < total_duration:
        # è¨ˆç®—ç†æƒ³åˆ‡åˆ†ä½ç½®
        ideal_end = current_start + target_duration

        if ideal_end >= total_duration:
            # å‰©é¤˜éƒ¨åˆ†ä¸è¶³ï¼Œç›´æ¥åˆ°å°¾ç«¯
            chunks.append((current_start, total_duration))
            break

        # åœ¨ [ideal_end - 30s, ideal_end + 30s] ç¯„åœå…§æ‰¾æœ€è¿‘çš„éœéŸ³æ®µ
        search_start = max(current_start + 60.0, ideal_end - 30.0)
        search_end = min(total_duration, current_start + max_duration)

        best_split = None
        best_distance = float("inf")

        for s_start, s_end in silence_regions:
            mid = (s_start + s_end) / 2
            if search_start <= mid <= search_end:
                dist = abs(mid - ideal_end)
                if dist < best_distance:
                    best_distance = dist
                    best_split = mid

        if best_split is not None:
            chunks.append((current_start, best_split))
            current_start = best_split
        else:
            # æ‰¾ä¸åˆ°éœéŸ³æ®µï¼Œå¼·åˆ¶åˆ‡åˆ†
            forced_end = min(current_start + max_duration, total_duration)
            chunks.append((current_start, forced_end))
            current_start = forced_end

    print(f"ğŸ“Š åˆ†æ®µçµæœï¼š{len(chunks)} å€‹ç‰‡æ®µ")
    for i, (start, end) in enumerate(chunks):
        print(f"   ç‰‡æ®µ {i+1}: {start:.1f}s â†’ {end:.1f}s ({end - start:.1f}s)")

    return chunks


# ============================================
# ASR å‡½å¼
# ============================================

def load_asr_model(max_new_tokens: int = 2048):
    """è¼‰å…¥ ASR æ¨¡å‹ï¼ˆå…±ç”¨ï¼‰"""
    model = Qwen3ASRModel.from_pretrained(
        "Qwen/Qwen3-ASR-1.7B",
        dtype=torch.bfloat16,
        device_map="cuda:0",
        max_inference_batch_size=32,
        max_new_tokens=max_new_tokens,
        forced_aligner="Qwen/Qwen3-ForcedAligner-0.6B",
        forced_aligner_kwargs=dict(
            dtype=torch.bfloat16,
            device_map="cuda:0",
        ),
    )
    return model


def asr(audio_path, model=None):
    """åŸ·è¡Œå–®æ®µ ASR è½‰éŒ„ä¸¦å›å‚³çµæœï¼ˆå«å­—å…ƒç´šæ™‚é–“æˆ³ï¼‰"""
    own_model = model is None
    if own_model:
        model = load_asr_model()

    results = model.transcribe(
        audio=[str(audio_path)],
        language=["Chinese"],
        return_time_stamps=True,
    )

    for r in results:
        print(f"  èªè¨€: {r.language}, æ–‡å­—é•·åº¦: {len(r.text)}")

    return results


def asr_chunked(audio_path, target_duration: float = 120.0):
    """
    åˆ†æ®µè™•ç†é•·éŸ³è¨Šçš„ ASR è½‰éŒ„ã€‚

    çŸ­éŸ³è¨Šï¼ˆâ‰¤ target_durationï¼‰ç›´æ¥æ•´æ®µè½‰éŒ„ï¼›
    é•·éŸ³è¨ŠæŒ‰éœéŸ³æ®µåˆ‡åˆ†å¾Œé€æ®µè½‰éŒ„ï¼Œä¿®æ­£æ™‚é–“æˆ³åç§»å†åˆä½µã€‚

    Args:
        audio_path: WAV éŸ³è¨Šè·¯å¾‘
        target_duration: åˆ‡åˆ†ç›®æ¨™é•·åº¦ï¼ˆç§’ï¼‰

    Returns:
        æ‰€æœ‰æ®µè½çš„ ASR çµæœåˆ—è¡¨ï¼ˆæ™‚é–“æˆ³å·²åç§»ä¿®æ­£ï¼‰
    """
    total_duration = get_audio_duration(str(audio_path))
    print(f"ğŸµ éŸ³è¨Šé•·åº¦: {total_duration:.1f}s")

    # çŸ­éŸ³è¨Šç›´æ¥æ•´æ®µè™•ç†
    if total_duration <= target_duration:
        print("   çŸ­éŸ³è¨Šï¼Œç›´æ¥æ•´æ®µè½‰éŒ„")
        model = load_asr_model()
        results = asr(audio_path, model=model)
        del model
        return results

    # é•·éŸ³è¨Šåˆ†æ®µè™•ç†
    chunks = split_audio_by_silence(str(audio_path), target_duration=target_duration)

    model = load_asr_model()
    all_results = []

    result_dir = Path(audio_path).parent

    # ä¸€æ¬¡è®€å–å®Œæ•´éŸ³è¨Š
    full_data, sr = sf.read(str(audio_path), dtype='float32')
    
    for i, (chunk_start, chunk_end) in enumerate(chunks):
        print(f"\nğŸ”„ è½‰éŒ„ç‰‡æ®µ {i+1}/{len(chunks)}: {chunk_start:.1f}s â†’ {chunk_end:.1f}s")

        # ç”¨ soundfile ç›´æ¥åˆ‡ç‰‡å¯«å‡ºå„æ®µ WAV
        chunk_path = result_dir / f"chunk_{i}.wav"
        start_sample = int(chunk_start * sr)
        end_sample = int(chunk_end * sr)
        chunk_data = full_data[start_sample:end_sample]
        sf.write(str(chunk_path), chunk_data, sr)

        # ASR è½‰éŒ„
        chunk_results = asr(str(chunk_path), model=model)

        # ä¿®æ­£æ™‚é–“æˆ³åç§»ï¼ˆForcedAlignItem æ˜¯ frozen dataclassï¼Œéœ€ç”¨ replaceï¼‰
        from dataclasses import replace as dc_replace
        for r in chunk_results:
            if hasattr(r, 'time_stamps') and r.time_stamps:
                new_timestamps = []
                for ts in r.time_stamps:
                    kwargs = {}
                    if hasattr(ts, 'start_time') and ts.start_time is not None:
                        kwargs['start_time'] = ts.start_time + chunk_start
                    if hasattr(ts, 'end_time') and ts.end_time is not None:
                        kwargs['end_time'] = ts.end_time + chunk_start
                    if kwargs:
                        new_timestamps.append(dc_replace(ts, **kwargs))
                    else:
                        new_timestamps.append(ts)
                r.time_stamps = new_timestamps

        all_results.extend(chunk_results)

        # æ¸…ç†æš«å­˜æª”
        try:
            chunk_path.unlink()
        except Exception:
            pass

    del model
    print(f"\nâœ… åˆ†æ®µè½‰éŒ„å®Œæˆï¼Œå…± {len(all_results)} æ®µçµæœ")
    return all_results


# ============================================
# ç¹é«”ä¸­æ–‡è½‰æ›
# ============================================

def convert_to_traditional(text: str) -> str:
    """å°‡ç°¡é«”ä¸­æ–‡è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰"""
    return cc.convert(text)


# ============================================
# èªè€…åˆ†é›¢
# ============================================

def diarization(audio_path):
    """åŸ·è¡Œèªè€…åˆ†é›¢ä¸¦å›å‚³ diarization å€æ®µåˆ—è¡¨"""
    from pyannote.audio import Pipeline
    import torch
    from audio_utils import load_audio

    pipeline = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-community-1",
        token="REMOVED_SECRET")

    pipeline.to(torch.device("cuda"))
    waveform, sample_rate = load_audio(str(audio_path))
    output = pipeline({"waveform": waveform, "sample_rate": sample_rate})

    # å–å¾—èªè€…åˆ†é›¢ Annotation ç‰©ä»¶
    diarization_result = output.speaker_diarization

    # æ”¶é›†èªè€…åˆ†é›¢å€æ®µ
    diar_segments = []
    for turn, _, speaker in diarization_result.itertracks(yield_label=True):
        diar_segments.append({
            "start": turn.start,
            "end": turn.end,
            "speaker": speaker,
        })
        print(f"{speaker} speaks between t={turn.start:.3f}s and t={turn.end:.3f}s")

    return diar_segments


# ============================================
# åˆä½µ ASR + èªè€…åˆ†é›¢
# ============================================

def merge_asr_with_diarization(
    asr_results,
    diar_segments: List[Dict],
    gap_threshold: float = 1.0,
    to_traditional: bool = True,
) -> List[Dict[str, Any]]:
    """
    å°‡ ASR å­—å…ƒç´šå°é½Šçµæœèˆ‡èªè€…åˆ†é›¢å€æ®µç²¾æº–åˆä½µã€‚

    æ ¸å¿ƒé‚è¼¯ï¼š
    1. å–å‡º ASR çµæœä¸­æ¯å€‹ ForcedAlignItem çš„å­—å…ƒèˆ‡æ™‚é–“æˆ³
    2. ä»¥æ¯å€‹å­—å…ƒçš„æ™‚é–“ä¸­é»åŒ¹é…è½å…¥çš„èªè€…åˆ†é›¢å€æ®µ
    3. é€£çºŒç›¸åŒèªè€…çš„å­—å…ƒåˆä½µç‚ºä¸€å€‹ç‰‡æ®µ
    4. ç›¸é„°åŒèªè€…ä¸”æ™‚é–“é–“éš” < gap_threshold çš„ç‰‡æ®µå†æ¬¡åˆä½µ
    5. å¯é¸ï¼šè½‰æ›ç‚ºç¹é«”ä¸­æ–‡

    Args:
        asr_results: ASR è½‰éŒ„çµæœåˆ—è¡¨ï¼ˆmodel.transcribe å›å‚³å€¼ï¼‰
        diar_segments: èªè€…åˆ†é›¢å€æ®µåˆ—è¡¨ [{"start", "end", "speaker"}, ...]
        gap_threshold: åˆä½µç›¸é„°åŒèªè€…ç‰‡æ®µçš„æœ€å¤§æ™‚é–“é–“éš”ï¼ˆç§’ï¼‰
        to_traditional: æ˜¯å¦è½‰æ›ç‚ºç¹é«”ä¸­æ–‡

    Returns:
        åˆä½µçµæœ [{"start", "end", "speaker", "text"}, ...]
    """
    if not diar_segments:
        print("âš ï¸ ç„¡èªè€…åˆ†é›¢è³‡æ–™ï¼Œæ‰€æœ‰æ–‡å­—æ¨™è¨˜ç‚º UNKNOWN")
        text = "".join(r.text for r in asr_results)
        if to_traditional:
            text = convert_to_traditional(text)
        return [{"start": 0.0, "end": 0.0, "speaker": "UNKNOWN", "text": text}]

    # --- Step 1: å–å‡ºæ‰€æœ‰å­—å…ƒçš„æ™‚é–“æˆ³ ---
    chars = []
    for r in asr_results:
        if not hasattr(r, "time_stamps") or not r.time_stamps:
            continue
        for ts in r.time_stamps:
            char_text = getattr(ts, "text", "")
            start_time = getattr(ts, "start_time", None)
            end_time = getattr(ts, "end_time", None)
            if start_time is None or end_time is None:
                continue
            chars.append({
                "text": char_text,
                "start": float(start_time),
                "end": float(end_time),
            })

    if not chars:
        print("âš ï¸ ASR çµæœä¸­ç„¡å­—å…ƒç´šæ™‚é–“æˆ³")
        text = "".join(r.text for r in asr_results)
        if to_traditional:
            text = convert_to_traditional(text)
        return [{"start": 0.0, "end": 0.0, "speaker": "UNKNOWN", "text": text}]

    print(f"ğŸ“Š å­—å…ƒæ•¸: {len(chars)}ï¼Œèªè€…å€æ®µæ•¸: {len(diar_segments)}")

    # --- Step 2: ç‚ºæ¯å€‹å­—å…ƒåŒ¹é…èªè€… ---
    import bisect
    diar_starts = [d["start"] for d in diar_segments]

    last_speaker = diar_segments[0]["speaker"]

    for char in chars:
        char_mid = (char["start"] + char["end"]) / 2
        speaker = None

        # äºŒåˆ†æœå°‹æ‰¾åˆ°å¯èƒ½åŒ¹é…çš„èªè€…å€æ®µ
        idx = bisect.bisect_right(diar_starts, char_mid) - 1
        if idx >= 0:
            for i in range(max(0, idx - 1), min(len(diar_segments), idx + 2)):
                d = diar_segments[i]
                if d["start"] <= char_mid <= d["end"]:
                    speaker = d["speaker"]
                    last_speaker = speaker
                    break

        # è‹¥ç„¡ç²¾ç¢ºåŒ¹é…ï¼Œæ‰¾æœ€æ¥è¿‘çš„èªè€…å€æ®µ
        if speaker is None:
            min_dist = float("inf")
            for d in diar_segments:
                if char_mid < d["start"]:
                    dist = d["start"] - char_mid
                elif char_mid > d["end"]:
                    dist = char_mid - d["end"]
                else:
                    dist = 0
                if dist < min_dist:
                    min_dist = dist
                    speaker = d["speaker"]
            if min_dist > 2.0:
                speaker = last_speaker
            else:
                last_speaker = speaker

        char["speaker"] = speaker

    # --- Step 3: æŒ‰èªè€…åˆ†çµ„ï¼ˆèªè€…è®ŠåŒ–æ™‚åˆ‡æ®µï¼‰---
    raw_segments = []
    current_speaker = chars[0]["speaker"]
    current_text = chars[0]["text"]
    current_start = chars[0]["start"]
    current_end = chars[0]["end"]

    for char in chars[1:]:
        if char["speaker"] == current_speaker:
            current_text += char["text"]
            current_end = char["end"]
        else:
            raw_segments.append({
                "start": current_start,
                "end": current_end,
                "speaker": current_speaker,
                "text": current_text,
            })
            current_speaker = char["speaker"]
            current_text = char["text"]
            current_start = char["start"]
            current_end = char["end"]

    # æœ€å¾Œä¸€æ®µ
    raw_segments.append({
        "start": current_start,
        "end": current_end,
        "speaker": current_speaker,
        "text": current_text,
    })

    # --- Step 4: åˆä½µç›¸é„°åŒèªè€…ä¸”æ™‚é–“æ¥è¿‘çš„ç‰‡æ®µ ---
    merged = [raw_segments[0].copy()]
    for seg in raw_segments[1:]:
        prev = merged[-1]
        time_gap = seg["start"] - prev["end"]
        if seg["speaker"] == prev["speaker"] and time_gap < gap_threshold:
            prev["end"] = seg["end"]
            prev["text"] += seg["text"]
        else:
            merged.append(seg.copy())

    # --- Step 5: éæ¿¾éçŸ­çš„é›œè¨Šç‰‡æ®µï¼ˆ<0.05 ç§’ä¸”æ–‡å­—ç‚ºç©ºï¼‰ ---
    final = []
    for seg in merged:
        duration = seg["end"] - seg["start"]
        text = seg["text"].strip()
        if duration < 0.05 and not text:
            continue
        final.append(seg)

    # --- Step 6: ç¹é«”ä¸­æ–‡è½‰æ› ---
    if to_traditional:
        for seg in final:
            seg["text"] = convert_to_traditional(seg["text"])

    print(f"âœ… åˆä½µå®Œæˆï¼š{len(chars)} å€‹å­—å…ƒ â†’ {len(raw_segments)} å€‹åŸå§‹ç‰‡æ®µ â†’ {len(final)} å€‹æœ€çµ‚ç‰‡æ®µ")

    return final


# ============================================
# å·¥å…·å‡½å¼
# ============================================

def format_time(seconds: float) -> str:
    """å°‡ç§’æ•¸æ ¼å¼åŒ–ç‚º HH:MM:SS.mmm"""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    if h > 0:
        return f"{h:02d}:{m:02d}:{s:06.3f}"
    return f"{m:02d}:{s:06.3f}"


# ============================================
# ä¸»ç¨‹å¼
# ============================================

if __name__ == "__main__":
    result_dir = RESULT_DIR / "test"
    result_dir.mkdir(parents=True, exist_ok=True)
    converted_path = result_dir / "converted.wav"
    convert_to_wav("audio.mp3", str(converted_path))

    # åŸ·è¡Œåˆ†æ®µ ASR
    asr_results = asr_chunked(converted_path)

    # åŸ·è¡Œèªè€…åˆ†é›¢
    diar_segments = diarization(converted_path)

    # åˆä½µçµæœï¼ˆå«ç¹é«”ä¸­æ–‡è½‰æ›ï¼‰
    merged = merge_asr_with_diarization(asr_results, diar_segments)

    # è¼¸å‡ºçµæœ
    print("\n" + "=" * 60)
    print("åˆä½µçµæœ")
    print("=" * 60)
    for seg in merged:
        start_str = format_time(seg["start"])
        end_str = format_time(seg["end"])
        print(f"[{start_str} â†’ {end_str}] {seg['speaker']}: {seg['text']}")

